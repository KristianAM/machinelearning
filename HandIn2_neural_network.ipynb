{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets\n",
    "You  must also try to do digit recognition using neural nets, in particular in Googles Open Source Framework, Tensorflow. Read the guide the TA gratiously has provided for you. \n",
    "\n",
    "### Deliverables\n",
    "You must try using a classific neural net with one input layer, one hidden layer, and one output layer.\n",
    "You can chosse whichever non-linear activation function you want but we suggest using logistic or rectified linear unit (relu).  You must again experiment with the hyperparameters, in particular\n",
    "network size (hidden layer) and regularization (weight decay, droupout), and use\n",
    "validation to find the best combination for the digits data. The validation data should come from the training data not the test data.\n",
    "\n",
    "## Report\n",
    "In the report, explain what you have done and and provide\n",
    "tables that show your results. Include tables of your validation\n",
    "results for all the SVMs and Neural Nets you have tested as well as the out \n",
    "of sample error/accuracy computed using the test set for the best SVM and the neural net you have found so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:20:35.978523\n",
      "n_layers: 300 prob: 0.9 reg_rate: 0.0001\n",
      "mean error 0.01348747015\n",
      "n_layers: 300 prob: 0.9 reg_rate: 1e-05\n",
      "mean error 0.0328516364098\n",
      "best params {'n_layers': 300, 'prob': 0.9, 'error': 0.013487470149993897, 'reg_rate': 0.0001}\n",
      "({'n_layers': 300, 'prob': 0.9, 'error': 0.013487470149993897, 'reg_rate': 0.0001}, 0.044186055660247803)\n",
      "sluttid  11:49:24.564854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "auTrain = np.load('auTrain.npz')\n",
    "auTrain_images = auTrain['digits']\n",
    "auTrain_labels = auTrain['labels']\n",
    "auTest = np.load('auTest.npz')\n",
    "auTest_images = auTest['digits']\n",
    "auTest_labels = auTest['labels']\n",
    "\n",
    "d = auTrain_images.shape[1]\n",
    "\n",
    "def plot_perf(names, fig=None, ax=None, xlim=None, ylim=None, figsize=(15, 10)):\n",
    "    import matplotlib\n",
    "    from IPython.display import display, clear_output\n",
    "\n",
    "    formats = ['%s: %%s' % n for n in names]\n",
    "    n = len(formats)\n",
    "\n",
    "    if ax is None or isinstance(ax, int):\n",
    "        if fig is None:\n",
    "            fig = matplotlib.figure.Figure(figsize=figsize)\n",
    "            fig.canvas = matplotlib.backends.backend_agg.FigureCanvasAgg(fig)\n",
    "\n",
    "        if isinstance(ax, int):\n",
    "            ax = fig.add_subplot(ax)\n",
    "        else:\n",
    "            ax = fig.gca()\n",
    "        if xlim is None:\n",
    "            ax.set_autoscalex_on(True)\n",
    "        else:\n",
    "            ax.set_autoscalex_on(False)\n",
    "            ax.set_xlim(*xlim)\n",
    "        if ylim is None:\n",
    "            ax.set_autoscaley_on(True)\n",
    "        else:\n",
    "            ax.set_autoscaley_on(False)\n",
    "            ax.set_ylim(*ylim)\n",
    "        ax.grid()\n",
    "    lines = [([], [], ax.plot([], [])[0]) for _ in range(n)]\n",
    "\n",
    "    def add(*points):\n",
    "        legend = ax.get_legend()\n",
    "        if legend is not None:\n",
    "            legend.remove()\n",
    "\n",
    "        for y, (xs, ys, line) in zip(points, lines):\n",
    "            xs.append(len(xs))\n",
    "            ys.append(y)\n",
    "            line.set_data(xs, ys)\n",
    "        ax.legend([f % p for f, p in zip(formats, points)])\n",
    "        ax.relim()\n",
    "        ax.autoscale(enable=None)\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "    return fig, add\n",
    "\n",
    "def permute_data_all():\n",
    "    assert auTrain_images.flags.c_contiguous\n",
    "    # Return a random permutation of the data\n",
    "    perm = np.random.permutation(len(auTrain_labels))\n",
    "    return auTrain_images[perm], auTrain_labels[perm]\n",
    "\n",
    "def neural_network(X_train, y_train, X_test, y_test, n_layers = 100, prob = 0.9, epochs = 500, batch_size = 16, reg_rate = 1e-4, plotting = True):\n",
    "    starttid =datetime.datetime.now().time()\n",
    "    auTrain_images = X_train\n",
    "    auTrain_labels = y_train\n",
    "    auTest_images = X_test\n",
    "    auTest_labels = y_test\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        data = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "        labels = tf.placeholder(tf.int32, shape = [None])\n",
    "        learning_rate = tf.placeholder(tf.float32)\n",
    "        #layer 1\n",
    "        W1 = tf.Variable(tf.truncated_normal([784, n_layers], stddev=0.1))\n",
    "        b1 = tf.Variable(tf.truncated_normal([n_layers], stddev=0.1))\n",
    "        y1 = tf.nn.relu(tf.matmul(data, W1) + b1)\n",
    "        \n",
    "        #dropoutlayer\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        drop = tf.nn.dropout(y1, keep_prob)\n",
    "\n",
    "        #Output\n",
    "        W2 = tf.Variable(tf.truncated_normal([n_layers, 10], stddev=0.1))\n",
    "        b2 = tf.Variable(tf.truncated_normal([10], stddev=0.1))\n",
    "        y2 = tf.nn.softmax(tf.matmul(drop, W2) + b2)\n",
    "\n",
    "        correct_prediction = tf.equal(labels, tf.cast(tf.argmax(y2, 1), labels.dtype))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        y2, labels)\n",
    "\n",
    "        \n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        reg = reg_rate * (tf.reduce_sum(W2 ** 2)+tf.reduce_sum(W1 ** 2)+tf.reduce_sum(b1 ** 2)+tf.reduce_sum(b2 ** 2))\n",
    "        # Minimization target is the sum of cross-entropy loss and regularization\n",
    "        target = loss + reg\n",
    "\n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "        train_step = opt.minimize(target)\n",
    "        fig, perf = plot_perf(names=['E_in', 'loss', 'E_test', 'reg'],\n",
    "                          ax=211, xlim=(0, epochs-1), ylim=(0, 0.7), figsize=(12, 10))\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.Session() as session:\n",
    "            session.run(tf.initialize_all_variables())\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                input_value, labels_value = permute_data_all()\n",
    "    \n",
    "                # Run train_step in mini-batches\n",
    "                for i in range(0, len(auTrain_labels), batch_size):\n",
    "                    j = min(len(auTrain_labels), i+batch_size)\n",
    "                    results = session.run(\n",
    "                        train_step,\n",
    "                        feed_dict={data: input_value[i:j],\n",
    "                                   labels: labels_value[i:j], keep_prob: prob,\n",
    "                                   learning_rate: 0.1})\n",
    "\n",
    "                train_accuracy = session.run(accuracy,feed_dict={data: auTrain_images, labels: auTrain_labels, keep_prob: 1.0})\n",
    "                correctly_labeled = session.run(correct_prediction, feed_dict={data: auTest_images, labels: auTest_labels, keep_prob: 1.0})\n",
    "                #print(correctly_labeled)\n",
    "                test_accuracy = session.run(accuracy, feed_dict={data: auTest_images, labels: auTest_labels, keep_prob: 1.0})\n",
    "                test_error = 1 - test_accuracy\n",
    "                #print(\"test_error \",test_error)\n",
    "                current_loss = session.run(loss, feed_dict={data: auTrain_images, labels: auTrain_labels, keep_prob: 1.0})\n",
    "                in_sample_error = 1 - train_accuracy\n",
    "                if plotting == True:\n",
    "                    perf(in_sample_error,  current_loss/3, test_error, session.run(reg)/3)\n",
    "\n",
    "            test_accuracy = session.run( accuracy,\n",
    "            feed_dict={data: auTest_images,\n",
    "                       labels: auTest_labels, keep_prob: 1.0})\n",
    "            test_error = 1 - test_accuracy\n",
    "            #print(test_error)\n",
    "            #print(\"starttid \",starttid,\" sluttid \",datetime.datetime.now().time())           \n",
    "            return(test_error)\n",
    "            \n",
    "def CV_nn(X_train, y_train, n_layers_list = [10,100,200,300], prob_list = [0.5,0.7,0.9], reg_rate_list = [0.001,1e-4,1e-5]):\n",
    "    split = ShuffleSplit(n_splits=5, test_size=0.2)\n",
    "    print(datetime.datetime.now().time())\n",
    "    res_list = []\n",
    "    best_error = 1.0\n",
    "    f=open('neuralresult.txt','w')\n",
    "    f.close()\n",
    "    f=open('neuralresult.txt','a')\n",
    "    for n_layers in n_layers_list:\n",
    "        for prob in prob_list:\n",
    "            for reg_rate in reg_rate_list:\n",
    "                f.write('n_layers: '+str(n_layers)+ ' prob: '+str(prob)+' reg_rate: '+ str(reg_rate)+\"\\n\")\n",
    "                print('n_layers:', n_layers, 'prob:', prob, 'reg_rate:', reg_rate)\n",
    "                class_list = []\n",
    "                for train_index, test_index in split.split(X_train):\n",
    "                    class_list.append(neural_network(X_train[train_index], y_train[train_index],\n",
    "                                                 X_train[test_index], y_train[test_index],\n",
    "                                                     n_layers = n_layers, prob = prob, epochs = 50,\n",
    "                                                     batch_size = 16, reg_rate = reg_rate, plotting = False))\n",
    "                mean_error = np.mean(class_list)\n",
    "                f.write('mean error '+str(mean_error)+\"\\n\")\n",
    "                print('mean error', mean_error)\n",
    "                res_dict = {'n_layers' : n_layers, 'prob' : prob, 'reg_rate':reg_rate, 'error' : mean_error}\n",
    "                res_list.append(res_dict)\n",
    "                if mean_error < best_error:\n",
    "                    best_error = mean_error\n",
    "                    best_params = res_dict\n",
    "\n",
    "    print(\"best params\",best_params) \n",
    "    f.write(str(best_params['n_layers']))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(best_params['prob']))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(best_params['reg_rate']))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(best_params['error']))\n",
    "    f.write(\"\\n\")\n",
    "    resulterror=neural_network(X_train, y_train,\n",
    "                               auTest_images, auTest_labels,\n",
    "                               n_layers = best_params['n_layers'], prob = best_params['prob'], epochs = 50,\n",
    "                               batch_size = 16, reg_rate = best_params['reg_rate'], plotting = False)\n",
    "    f.write(\"test result \")\n",
    "    f.write(str(resulterror))\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "    return best_params,resulterror\n",
    "if __name__ == '__main__':\n",
    "    #print(neural_network(auTrain_images, auTrain_labels, auTest_images, auTest_labels))\n",
    "    print (CV_nn(auTrain_images, auTrain_labels))\n",
    "    print(\"sluttid \",datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "11:13:58.371865\n",
    "n_layers: 10 prob: 0.5 reg_rate: 1e-05\n",
    "mean error 0.162909448147\n",
    "n_layers: 10 prob: 0.5 reg_rate: 0.0001\n",
    "mean error 0.147591531277\n",
    "n_layers: 10 prob: 0.5 reg_rate: 0.001\n",
    "mean error 0.136994206905\n",
    "n_layers: 10 prob: 0.7 reg_rate: 1e-05\n",
    "mean error 0.106069374084\n",
    "n_layers: 10 prob: 0.7 reg_rate: 0.0001\n",
    "mean error 0.102793836594\n",
    "n_layers: 10 prob: 0.7 reg_rate: 0.001\n",
    "mean error 0.127841997147\n",
    "n_layers: 10 prob: 0.9 reg_rate: 1e-05\n",
    "mean error 0.110789990425\n",
    "n_layers: 10 prob: 0.9 reg_rate: 0.0001\n",
    "mean error 0.0780346870422\n",
    "n_layers: 10 prob: 0.9 reg_rate: 0.001\n",
    "mean error 0.0989402770996\n",
    "n_layers: 100 prob: 0.5 reg_rate: 1e-05\n",
    "mean error 0.0438342928886\n",
    "n_layers: 100 prob: 0.5 reg_rate: 0.0001\n",
    "mean error 0.0476878643036\n",
    "n_layers: 100 prob: 0.5 reg_rate: 0.001\n",
    "mean error 0.0735067486763\n",
    "n_layers: 100 prob: 0.7 reg_rate: 1e-05\n",
    "mean error 0.0459537625313\n",
    "n_layers: 100 prob: 0.7 reg_rate: 0.0001\n",
    "mean error 0.0290944337845\n",
    "n_layers: 100 prob: 0.7 reg_rate: 0.001\n",
    "mean error 0.0626204371452\n",
    "n_layers: 100 prob: 0.9 reg_rate: 1e-05\n",
    "mean error 0.0409441232681\n",
    "n_layers: 100 prob: 0.9 reg_rate: 0.0001\n",
    "mean error 0.0214836359024\n",
    "n_layers: 100 prob: 0.9 reg_rate: 0.001\n",
    "mean error 0.0514451026917\n",
    "n_layers: 200 prob: 0.5 reg_rate: 1e-05\n",
    "mean error 0.0318882465363\n",
    "n_layers: 200 prob: 0.5 reg_rate: 0.0001\n",
    "mean error 0.0304431796074\n",
    "n_layers: 200 prob: 0.5 reg_rate: 0.001\n",
    "mean error 0.0629094362259\n",
    "n_layers: 200 prob: 0.7 reg_rate: 1e-05\n",
    "mean error 0.0198458552361\n",
    "n_layers: 200 prob: 0.7 reg_rate: 0.0001\n",
    "mean error 0.019267821312\n",
    "n_layers: 200 prob: 0.7 reg_rate: 0.001\n",
    "mean error 0.05838149786\n",
    "n_layers: 200 prob: 0.9 reg_rate: 1e-05\n",
    "mean error 0.0164739847183\n",
    "n_layers: 200 prob: 0.9 reg_rate: 0.0001\n",
    "mean error 0.0158959388733\n",
    "n_layers: 200 prob: 0.9 reg_rate: 0.001\n",
    "mean error 0.0506743907928\n",
    "n_layers: 300 prob: 0.5 reg_rate: 1e-05\n",
    "mean error 0.0224470019341\n",
    "n_layers: 300 prob: 0.5 reg_rate: 0.0001\n",
    "mean error 0.0439306378365\n",
    "n_layers: 300 prob: 0.5 reg_rate: 0.001\n",
    "mean error 0.0622350692749\n",
    "n_layers: 300 prob: 0.7 reg_rate: 1e-05\n",
    "mean error 0.0354527950287\n",
    "n_layers: 300 prob: 0.7 reg_rate: 0.0001\n",
    "mean error 0.0586705207825\n",
    "n_layers: 300 prob: 0.7 reg_rate: 0.001\n",
    "mean error 0.0526011705399\n",
    "n_layers: 300 prob: 0.9 reg_rate: 1e-05\n",
    "mean error 0.0328516364098\n",
    "n_layers: 300 prob: 0.9 reg_rate: 0.0001\n",
    "mean error 0.01348747015\n",
    "n_layers: 300 prob: 0.9 reg_rate: 0.001\n",
    "mean error 0.0439306378365\n",
    "best params {'n_layers': 300, 'prob': 0.9, 'error': 0.013487470149993897, 'reg_rate': 0.0001}\n",
    "({'n_layers': 300, 'prob': 0.9, 'error': 0.013487470149993897, 'reg_rate': 0.0001})\n",
    "Test result: 0.044186055660247803\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
